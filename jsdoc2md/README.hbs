# Simple web crawler for node.js

[![NPM version](https://img.shields.io/npm/v/simplecrawler.svg)](https://www.npmjs.com/package/simplecrawler)
[![Linux Build Status](https://img.shields.io/travis/simplecrawler/simplecrawler/master.svg)](https://travis-ci.org/simplecrawler/simplecrawler)
[![Windows Build Status](https://img.shields.io/appveyor/ci/simplecrawler/simplecrawler/master.svg?label=Windows%20build)](https://ci.appveyor.com/project/simplecrawler/simplecrawler/branch/master)
[![Dependency Status](https://img.shields.io/david/simplecrawler/simplecrawler.svg)](https://david-dm.org/simplecrawler/simplecrawler)
[![devDependency Status](https://img.shields.io/david/dev/simplecrawler/simplecrawler.svg)](https://david-dm.org/simplecrawler/simplecrawler#info=devDependencies)

simplecrawler is designed to provide a basic, flexible and robust API for crawling websites. It was written to archive, analyse, and search some very large websites and has happily chewed through hundreds of thousands of pages and written tens of gigabytes to disk without issue.

## What does simplecrawler do?

* Provides a very simple event driven API using `EventEmitter`
* Extremely configurable base for writing your own crawler
* Provides some simple logic for auto-detecting linked resources - which you can replace or augment
* Automatically respects any robots.txt rules
* Has a flexible queue system which can be frozen to disk and defrosted
* Provides basic statistics on network performance
* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)

## Documentation

- [Installation](#installation)
- [Getting started](#getting-started)
- [Events](#events)
    - [A note about HTTP error conditions](#a-note-about-http-error-conditions)
    - [Waiting for asynchronous event listeners](#waiting-for-asynchronous-event-listeners)
- [Configuration](#configuration)
- [Fetch conditions](#fetch-conditions)
- [Download conditions](#download-conditions)
- [The queue](#the-queue)
    - [Manually adding to the queue](#manually-adding-to-the-queue)
    - [Queue items](#queue-items)
    - [Queue statistics and reporting](#queue-statistics-and-reporting)
    - [Saving and reloading the queue (freeze/defrost)](#saving-and-reloading-the-queue-freezedefrost)
- [Cookies](#cookies)
    - [Cookie events](#cookie-events)
- [Link Discovery](#link-discovery)
- [FAQ/Troubleshooting](#faqtroubleshooting)
- [Node Support Policy](#node-support-policy)
- [Current Maintainers](#current-maintainers)
- [Contributing](#contributing)
- [Contributors](#contributors)
- [License](#license)

## Installation

```sh
npm install --save simplecrawler
```

## Getting Started

Initializing simplecrawler is a simple process. First, you require the module and instantiate it with a single argument. You then configure the properties you like (eg. the request interval), register a few event listeners, and call the start method. Let's walk through the process!

After requiring the crawler, we create a new instance of it. We supply the constructor with a URL that indicates which domain to crawl and which resource to fetch first.

```js
var Crawler = require("simplecrawler");

var crawler = new Crawler("http://www.example.com/");
```

You can initialize the crawler with or without the `new` operator. Being able to skip it comes in handy when you want to chain API calls.

```js
var crawler = Crawler("http://www.example.com/")
    .on("fetchcomplete", function () {
        console.log("Fetched a resource!")
    });
```

By default, the crawler will only fetch resources on the same domain as that in the URL passed to the constructor. But this can be changed through the {{>link to="Crawler#domainWhitelist" html=true caption="crawler.domainWhitelist"}} property.

Now, let's configure some more things before we start crawling. Of course, you're probably wanting to ensure you don't take down your web server. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250 ms like this:

```js
crawler.interval = 10000; // Ten seconds
crawler.maxConcurrency = 3;
```

You can also define a max depth for links to fetch:

```js
crawler.maxDepth = 1; // Only first page is fetched (with linked CSS & images)
// Or:
crawler.maxDepth = 2; // First page and discovered links from it are fetched
// Or:
crawler.maxDepth = 3; // Etc.
```

For a full list of configurable properties, see the [configuration section](#configuration).

You'll also need to set up event listeners for the [events](#events) you want to listen to. {{>link to="Crawler#fetchcomplete" html=true caption="crawler.fetchcomplete"}} and {{>link to="Crawler#complete" html=true caption="crawler.complete"}} are good places to start.

```js
crawler.on("fetchcomplete", function(queueItem, responseBuffer, response) {
    console.log("I just received %s (%d bytes)", queueItem.url, responseBuffer.length);
    console.log("It was a resource of type %s", response.headers['content-type']);
});
```

Then, when you're satisfied and ready to go, start the crawler! It'll run through its queue finding linked resources on the domain to download, until it can't find any more.

```js
crawler.start();
```

## Events

simplecrawler's API is event driven, and there are plenty of events emitted during the different stages of the crawl.

{{#identifiers memberof="Crawler" kind="event"}}
{{>docs}}
{{/identifiers}}

### A note about HTTP error conditions

By default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on("data",function(chunk) {...})`) you can save the information yourself.

### Waiting for asynchronous event listeners

Sometimes, you might want to wait for simplecrawler to wait for you while you perform some asynchronous tasks in an event listener, instead of having it racing off and firing the `complete` event, halting your crawl. For example, if you're doing your own link discovery using an asynchronous library method.

simplecrawler provides a `wait` method you can call at any time. It is available via `this` from inside listeners, and on the crawler object itself. It returns a callback function.

Once you've called this method, simplecrawler will not fire the `complete` event until either you execute the callback it returns, or a timeout is reached (configured in `crawler.listenerTTL`, by default 10000 ms.)

#### Example asynchronous event listener

```js
crawler.on("fetchcomplete", function(queueItem, data, res) {
    var continue = this.wait();

    doSomeDiscovery(data, function(foundURLs) {
        foundURLs.forEach(function(url) {
            crawler.queueURL(url, queueItem);
        });

        continue();
    });
});
```

## Configuration

simplecrawler is highly configurable and there's a long list of settings you can change to adapt it to your specific needs.

{{#identifiers memberof="Crawler" kind="member"}}
{{>docs}}
{{/identifiers}}

## Fetch conditions

simplecrawler has an concept called fetch conditions that offers a flexible API for filtering discovered resources before they're put in the queue. A fetch condition is a function that takes a queue item candidate and evaluates (synchronously or asynchronously) whether it should be added to the queue or not. *Please note: with the next major release, all fetch conditions will be asynchronous.*

You may add as many fetch conditions as you like, and remove them at runtime. simplecrawler will evaluate every fetch condition in parallel until one is encountered that returns a falsy value. If that happens, the resource in question will not be fetched.

This API is complemented by [download conditions](#download-conditions) that determine whether a resource's body data should be downloaded.

{{#identifier id="Crawler#addFetchCondition"}}
{{>docs}}
{{/identifier}}
{{#identifier id="Crawler~addFetchConditionCallback"}}
{{>docs}}
{{/identifier}}
{{#identifier id="Crawler#removeFetchCondition"}}
{{>docs}}
{{/identifier}}

## Download conditions

While fetch conditions let you determine which resources to put in the queue, download conditions offer the same kind of flexible API for determining which resources' data to download. Download conditions support both a synchronous and an asynchronous API, but *with the next major release, all download conditions will be asynchronous.*

Download conditions are evaluated after the headers of a resource have been downloaded, if that resource returned an HTTP status between 200 and 299. This lets you inspect the content-type and content-length headers, along with all other properties on the queue item, before deciding if you want this resource's data or not.

{{#identifier id="Crawler#addDownloadCondition"}}
{{>docs}}
{{/identifier}}
{{#identifier id="Crawler~addDownloadConditionCallback"}}
{{>docs}}
{{/identifier}}
{{#identifier id="Crawler#removeDownloadCondition"}}
{{>docs}}
{{/identifier}}

## The queue

Like any other web crawler, simplecrawler has a queue. It can be directly accessed through {{>link to="Crawler#queue" html=true caption="crawler.queue"}} and implements an asynchronous interface for accessing queue items and statistics. There are several methods for interacting with the queue, the simplest being {{>link to="FetchQueue#get" html=true caption="crawler.queue.get"}}, which lets you get a queue item at a specific index in the queue.

{{#identifier id="FetchQueue#get"}}
{{>docs}}
{{/identifier}}

*All queue method are in reality synchronous by default, but simplecrawler is built to be able to use different queues that implement the same interface, and those implementations can be asynchronous - which means they could eg. be backed by a database.*

### Manually adding to the queue

To add items to the queue, use {{>link to="Crawler#queueURL" html=true caption="crawler.queueURL"}}.

{{#identifier id="Crawler#queueURL"}}
{{>docs}}
{{/identifier}}

### Queue items

Because when working with simplecrawler, you'll constantly be handed queue items, it helps to know what's inside them. Here's the formal documentation of the properties that they contain.

{{#identifier id="QueueItem"}}
{{>docs}}
{{/identifier}}

### Queue statistics and reporting

First of all, the queue can provide some basic statistics about the network performance of your crawl so far. This is done live, so don't check it 30 times a second. You can test the following properties:

* `requestTime`
* `requestLatency`
* `downloadTime`
* `contentLength`
* `actualDataSize`

You can get the maximum, minimum, and average values for each with the {{>link to="FetchQueue#max" html=true caption="crawler.queue.max"}}, {{>link to="FetchQueue#min" html=true caption="crawler.queue.min"}}, and {{>link to="FetchQueue#avg" html=true caption="crawler.queue.avg"}} functions respectively.

{{#identifier id="FetchQueue#max"}}
{{>docs}}
{{/identifier}}
{{#identifier id="FetchQueue#min"}}
{{>docs}}
{{/identifier}}
{{#identifier id="FetchQueue#avg"}}
{{>docs}}
{{/identifier}}

For general filtering or counting of queue items, there are two methods: {{>link to="FetchQueue#filterItems" html=true caption="crawler.queue.filterItems"}} and {{>link to="FetchQueue#countItems" html=true caption="crawler.queue.countItems"}}. Both take an object comparator and a callback.

{{#identifier id="FetchQueue#filterItems"}}
{{>docs}}
{{/identifier}}
{{#identifier id="FetchQueue#countItems"}}
{{>docs}}
{{/identifier}}

The object comparator can also contain other objects, so you may filter queue items based on properties in their `stateData` object as well.

```js
crawler.queue.filterItems({
    stateData: { code: 301 }
}, function(error, items) {
    console.log("These items returned a 301 HTTP status", items);
});
```

### Saving and reloading the queue (freeze/defrost)

It can be convenient to be able to save the crawl progress and later be able to reload it if your application fails or you need to abort the crawl for some reason. The `crawler.queue.freeze` and `crawler.queue.defrost` methods will let you do this.

**A word of warning** - they are not CPU friendly as they rely on `JSON.parse` and `JSON.stringify`. Use them only when you need to save the queue - don't call them after every request or your application's performance will be incredibly poor - they block like *crazy*. That said, using them when your crawler commences and stops is perfectly reasonable.

Note that the methods themselves are asynchronous, so if you are going to exit the process after you do the freezing, make sure you wait for callback - otherwise you'll get an empty file.

{{#identifier id="FetchQueue#freeze"}}
{{>docs}}
{{/identifier}}
{{#identifier id="FetchQueue#defrost"}}
{{>docs}}
{{/identifier}}

## Cookies

simplecrawler has an internal cookie jar, which collects and resends cookies automatically and by default. If you want to turn this off, set the {{>link to="Crawler#acceptCookies" html=true caption="crawler.acceptCookies"}} option to `false`. The cookie jar is accessible via {{>link to="Crawler#cookies" html=true caption="crawler.cookies"}}, and is an event emitter itself.

### Cookie events

{{#identifiers memberof="CookieJar" kind="event"}}
{{>docs}}
{{/identifiers}}

## Link Discovery

simplecrawler's discovery function is made to be replaceable — you can easily write your own that discovers only the links you're interested in.

The method must accept a buffer and a [`queueItem`](#queue-items), and return the resources that are to be added to the queue.

It is quite common to pair simplecrawler with a module like [cheerio](https://npmjs.com/package/cheerio) that can correctly parse HTML and provide a DOM like API for querying — or even a whole headless browser, like phantomJS.

The example below demonstrates how one might achieve basic HTML-correct discovery of only link tags using cheerio.

```js
crawler.discoverResources = function(buffer, queueItem) {
    var $ = cheerio.load(buffer.toString("utf8"));

    return $("a[href]").map(function () {
        return $(this).attr("href");
    }).get();
};
```

## FAQ/Troubleshooting

There are a couple of questions that pop up more often than others in the issue tracker. If you're having trouble with simplecrawler, please have a look at the list below before submitting an issue.

- **Q: Why does simplecrawler discover so many invalid URLs?**

    A: simplecrawler's built-in discovery method is purposefully naive - it's a brute force approach intended to find everything: URLs in comments, binary files, scripts, image EXIF data, inside CSS documents, and more — useful for archiving and use cases where it's better to have false positives than fail to discover a resource.

    It's definitely not a solution for every case, though — if you're writing a link checker or validator, you don't want erroneous 404s throwing errors. Therefore, simplecrawler allows you to tune discovery in a few key ways:

    - You can either add to (or remove from) the {{>link to="Crawler#discoverRegex" html=true caption="crawler.discoverRegex"}} array, tweaking the search patterns to meet your requirements; or
    - Swap out the `discoverResources` method. Parsing HTML pages is beyond the scope of simplecrawler, but it is very common to combine simplecrawler with a module like [cheerio](https://npmjs.com/package/cheerio) for more sophisticated resource discovery.

    Further documentation is available in the [link discovery](#link-discovery) section.

- **Q: Why did simplecrawler complete without fetching any resources?**

    A: When this happens, it is usually because the initial request was redirected to a different domain that wasn't in the {{>link to="Crawler#domainWhitelist" html=true caption="crawler.domainWhitelist"}}.

- **Q: How do I crawl a site that requires a login?**

    A: Logging in to a site is usually fairly simple and most login procedures look alike. We've included an example that covers a lot of situations, but sadly, there isn't a one true solution for how to deal with logins, so there's no guarantee that this code works right off the bat.

    What we do here is:
    1. fetch the login page,
    2. store the session cookie assigned to us by the server,
    3. extract any CSRF tokens or similar parameters required when logging in,
    4. submit the login credentials.

    ```js
    var Crawler = require("simplecrawler"),
        url = require("url"),
        cheerio = require("cheerio"),
        request = require("request");

    var initialURL = "https://example.com/";

    var crawler = new Crawler(initialURL);

    request("https://example.com/login", {
        // The jar option isn't necessary for simplecrawler integration, but it's
        // the easiest way to have request remember the session cookie between this
        // request and the next
        jar: true
    }, function (error, response, body) {
        // Start by saving the cookies. We'll likely be assigned a session cookie
        // straight off the bat, and then the server will remember the fact that
        // this session is logged in as user "iamauser" after we've successfully
        // logged in
        crawler.cookies.addFromHeaders(response.headers["set-cookie"]);

        // We want to get the names and values of all relevant inputs on the page,
        // so that any CSRF tokens or similar things are included in the POST
        // request
        var $ = cheerio.load(body),
            formDefaults = {},
            // You should adapt these selectors so that they target the
            // appropriate form and inputs
            formAction = $("#login").attr("action"),
            loginInputs = $("input");

        // We loop over the input elements and extract their names and values so
        // that we can include them in the login POST request
        loginInputs.each(function(i, input) {
            var inputName = $(input).attr("name"),
                inputValue = $(input).val();

            formDefaults[inputName] = inputValue;
        });

        // Time for the login request!
        request.post(url.resolve(initialURL, formAction), {
            // We can't be sure that all of the input fields have a correct default
            // value. Maybe the user has to tick a checkbox or something similar in
            // order to log in. This is something you have to find this out manually
            // by logging in to the site in your browser and inspecting in the
            // network panel of your favorite dev tools what parameters are included
            // in the request.
            form: Object.assign(formDefaults, {
                username: "iamauser",
                password: "supersecretpw"
            }),
            // We want to include the saved cookies from the last request in this
            // one as well
            jar: true
        }, function (error, response, body) {
            // That should do it! We're now ready to start the crawler
            crawler.start();
        });
    });

    crawler.on("fetchcomplete", function (queueItem, responseBuffer, response) {
        console.log("Fetched", queueItem.url, responseBuffer.toString());
    });
    ```

- **Q: What does it mean that events are asynchronous?**

    A: One of the core concepts of node.js is its asynchronous nature. I/O operations (like network requests) take place outside of the main thread (which is where your code is executed). This is what makes node fast, the fact that it can continue executing code while there are multiple HTTP requests in flight, for example. But to be able to get back the result of the HTTP request, we need to register a function that will be called when the result is ready. This is what *asynchronous* means in node - the fact that code can continue executing while I/O operations are in progress - and it's the same concept as with AJAX requests in the browser.

- **Q: Promises are nice, can I use them with simplecrawler?**

    A: No, not really. Promises are meant as a replacement for callbacks, but simplecrawler is event driven, not callback driven. Using callbacks to any greater extent in simplecrawler wouldn't make much sense, since you normally need to react more than once to what happens in simplecrawler.

- **Q: Something's happening and I don't see the output I'm expecting!**

    Before filing an issue, check to see that you're not just missing something by logging *all* crawler events with the code below:

    ```js
    var originalEmit = crawler.emit;
    crawler.emit = function(evtName, queueItem) {
        crawler.queue.countItems({ fetched: true }, function(err, completeCount) {
            if (err) {
                throw err;
            }

            crawler.queue.getLength(function(err, length) {
                if (err) {
                    throw err;
                }

                console.log("fetched %d of %d — %d open requests, %d open listeners",
                    completeCount,
                    length,
                    crawler._openRequests.length,
                    crawler._openListeners);
            });
        });

        console.log(evtName, queueItem ? queueItem.url ? queueItem.url : queueItem : null);
        originalEmit.apply(crawler, arguments);
    };
    ```

    If you don't see what you need after inserting that code block, and you still need help, please attach the output of all the events fired with your email/issue.

## Node Support Policy

Simplecrawler will officially support stable and LTS versions of Node which are currently supported by the Node Foundation.

Currently supported versions:

- 4.x
- 5.x
- 6.x
- 7.x

## Current Maintainers

* [Christopher Giffard](https://github.com/cgiffard)
* [Fredrik Ekelund](https://github.com/fredrikekelund)
* [Konstantin Bläsi](https://github.com/konstantinblaesi)
* [XhmikosR](https://github.com/XhmikosR)

## Contributing

Please see the [contributor guidelines](https://github.com/simplecrawler/simplecrawler/blob/master/CONTRIBUTING.md) before submitting a pull request to ensure that your contribution is able to be accepted quickly and easily!

## Contributors

simplecrawler has benefited from the kind efforts of dozens of contributors, to whom we are incredibly grateful. We originally listed their individual contributions but it became pretty unwieldy - the [full list can be found here.](https://github.com/simplecrawler/simplecrawler/graphs/contributors)

## License

Copyright (c) 2017, Christopher Giffard.

All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.
* Redistributions in binary form must reproduce the above copyright notice, this
  list of conditions and the following disclaimer in the documentation and/or
  other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
